{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "\n",
    "#### Michelle Appel (10170359)\n",
    "#### Nils Hulzeboch (10749411)\n",
    "\n",
    "#### 16-01-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Part [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hypothesis Testing – The problem of multiple comparisons [ 5 points ]\n",
    "Experimentation in AI often happens like this:\n",
    "A. Modify/Build an algorithm\n",
    "B. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "C. If not significant, go back to step A\n",
    "D. If significant, start writing a paper.\n",
    "Compute the probabilities below  How many hypothesis tests, m, does it take to get to  (with Type I error for each test = α):\n",
    "(a) P(mt  h  experiment gives significant result | m experiments lacking power to reject H 0)  ? (b) P(at least one significant result | m experiments lacking power to reject H 0)  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$) = $1 - (1 - \\alpha)^m$ (= $m \\alpha$ when $\\alpha$ is small)\n",
    "\n",
    "Where m is the amount of experiments and $\\alpha$ is the Type I error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) P(at least one significant result | m experiments lacking power to reject $H_0$) = $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bias and unfairness in Interleaving experiments [ 10 points ]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning 2⁄3 of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a\n",
    "situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Part [85 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevances = ('N', 'R', 'HR') # The three relevance classes\n",
    "\n",
    "# All combinations with length 5 of the relevance classes\n",
    "combinations = list(itertools.combinations_with_replacement(relevances, 5))\n",
    "\n",
    "# All permutations per combination\n",
    "permutations = ()\n",
    "for combination in combinations:\n",
    "    permutations += tuple(set(itertools.permutations(combination)))\n",
    "    \n",
    "# Ranking pairs of production P and experimental E\n",
    "ranking_pairs = ()\n",
    "for ranking_p in permutations:\n",
    "    for ranking_e in permutations:\n",
    "        if ranking_p != ranking_e: # If pairs are not the same\n",
    "            ranking_pairs += ((ranking_p, ranking_e),) # Extend list with ranking pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Implement Evaluation Measures (10 points)\n",
    "    \n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection – pay extra attention on how to find this)\n",
    "\n",
    "##### Binary evaluation measures:\n",
    "1. Precision at rank k,\n",
    "2. Recall at rank k,\n",
    "3. Average Precision,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "def precision(ranking, rank=None):\n",
    "    tp = ranking[:rank].count('R') + ranking[:rank].count('HR')\n",
    "    fp = ranking[:rank].count('N')\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "# Recall = TP / (TP + FN)\n",
    "def recall(ranking, no_relevant_documents, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    tp = ranking[:rank].count('R') + ranking[:rank].count('HR')\n",
    "    fn = no_relevant_documents - tp\n",
    "    return tp / (tp + fn)\n",
    "    \n",
    "def recalls(ranking_pair, rank=None):\n",
    "    no_relevant_documents = 0\n",
    "    for ranking in ranking_pair:\n",
    "        no_relevant_documents += ranking.count('R') + ranking.count('HR')\n",
    "    \n",
    "    recalls = ()    \n",
    "    for ranking in ranking_pair:\n",
    "        recalls += (recall(ranking=ranking, no_relevant_documents=no_relevant_documents, rank=rank),)\n",
    "    return recalls\n",
    "\n",
    "\n",
    "# Average precision\n",
    "def average_precision(ranking):\n",
    "    precisions = ()\n",
    "    for rank in range(1, len(ranking)+1):\n",
    "        if ranking[rank-1] == 'R' or ranking[rank-1] == 'HR':\n",
    "            precisions += (precision(ranking, rank=rank),)\n",
    "\n",
    "    if len(precisions) > 0:\n",
    "        return np.mean(precisions)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-graded evaluation measures:\n",
    "\n",
    "1. Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "2. Expected Reciprocal Rank (ERR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "def nDCGk(ranking, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    gains = ()\n",
    "    for r, rel_grade in enumerate(ranking[:rank]):\n",
    "        if rel_grade == 'N':\n",
    "            rel_r = 0\n",
    "        elif rel_grade == 'R':\n",
    "            rel_r = 0.5\n",
    "        elif rel_grade == 'HR':\n",
    "            rel_r = 1\n",
    "        \n",
    "        gains += ((2**rel_r - 1)/(np.log2(2+r)),)\n",
    "        \n",
    "    return np.sum(gains)\n",
    "\n",
    "\n",
    "# Mapping from relevance grades to probability of relevance\n",
    "def R(rel_grade, g_max):\n",
    "    if rel_grade == 'N':\n",
    "        g = 0\n",
    "    elif rel_grade == 'R':\n",
    "        g = 0.5\n",
    "    elif rel_grade == 'HR':\n",
    "        g = 1\n",
    "\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "# Expected Reciprocal Rank (ERR)\n",
    "def ERR(ranking, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    g_max = 1\n",
    "    \n",
    "    return np.sum([([np.prod(((1/(r+1))*(1 - R(i_rel_grad, g_max))*R(r_rel_grad, g_max),)) \n",
    "                for i_rel_grad in ranking[:rank]],) \n",
    "                for r, r_rel_grad in enumerate(ranking[:rank])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the 𝛥measure (0 points)\n",
    "    \n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure = measureE-measureP. Consider only those pairs for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_measure(ranking_pairs, evaluation_measure):\n",
    "    delta_measures = ()\n",
    "    \n",
    "    for ranking_pair in ranking_pairs:\n",
    "        delta_measure = evaluation_measure(ranking_pair[1]) - evaluation_measure(ranking_pair[0])\n",
    "        if delta_measure > 0:\n",
    "            delta_measures += (delta_measure,)\n",
    "            \n",
    "    return np.mean(delta_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Based on Lecture 2]\n",
    "#### Step 4: Implement Interleaving (15 points)\n",
    "\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Balanced Interleaving\n",
    "def balanced_interleaving(ranking_pair, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking_pair[0]) + len(ranking_pair[1])\n",
    "    \n",
    "    p_first = random.randint(0,1)\n",
    "    \n",
    "    interleaved_ranking = ()\n",
    "    picking_order = ()\n",
    "    for p, e in zip(*ranking_pair):\n",
    "        if p_first:\n",
    "            interleaved_ranking += (p, e)\n",
    "            picking_order += ('P', 'E')\n",
    "        else:\n",
    "            interleaved_ranking += (e, p)\n",
    "            picking_order += ('E', 'P')\n",
    "            \n",
    "    return interleaved_ranking[:rank], picking_order\n",
    "       \n",
    "# Team-Draft Interleaving\n",
    "def team_draft_interleaving(ranking_pair, rank=None):\n",
    "    ranking_p = list(ranking_pair[0])\n",
    "    ranking_e = list(ranking_pair[1])\n",
    "                             \n",
    "    if rank is None:\n",
    "        rank = len(ranking_p) + len(ranking_e)\n",
    "        \n",
    "    team_p = ()\n",
    "    team_e = ()\n",
    "        \n",
    "    interleaved_ranking = ()\n",
    "    picking_order = ()\n",
    "    for i in range(rank):\n",
    "        if (len(team_p) < len(team_e)) or (len(team_p) == len(team_e) and random.randint(0,1)):\n",
    "            rel_grade = ranking_p.pop(0)\n",
    "            interleaved_ranking += (rel_grade,)\n",
    "            team_p += (rel_grade,)\n",
    "            picking_order += ('P',)\n",
    "        else:\n",
    "            rel_grade = ranking_e.pop(0)\n",
    "            interleaved_ranking += (rel_grade,)\n",
    "            team_e += (rel_grade,)\n",
    "            picking_order += ('E',)\n",
    "    \n",
    "    return interleaved_ranking[:rank], picking_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: (('HR', 'R', 'N', 'N', 'R'), ('N', 'N', 'R', 'N', 'R')) \n",
      "\n",
      "Balanced interleaving:\n",
      "('HR', 'N', 'R', 'N', 'N', 'R', 'N', 'N', 'R', 'R')\n",
      "('P', 'E', 'P', 'E', 'P', 'E', 'P', 'E', 'P', 'E') (order of picking)\n",
      "\n",
      "Team draft interleaving:\n",
      "('HR', 'N', 'R', 'N', 'N', 'R', 'N', 'N', 'R', 'R')\n",
      "('P', 'E', 'P', 'E', 'P', 'E', 'E', 'P', 'P', 'E') (order of picking)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test interleaving methods with an example\n",
    "test_pair = (('HR', 'R', 'N', 'N', 'R'), ('N', 'N', 'R', 'N', 'R'))\n",
    "print('Example:', test_pair, '\\n')\n",
    "\n",
    "bal_inter, bal_order = balanced_interleaving(test_pair)\n",
    "print('Balanced interleaving:')\n",
    "print(bal_inter)\n",
    "print(bal_order, '(order of picking)\\n')\n",
    "      \n",
    "team_inter, team_order = team_draft_interleaving(test_pair)\n",
    "print('Team draft interleaving:')\n",
    "print(team_inter)\n",
    "print(team_order, '(order of picking)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Implement User Clicks Simulation (15 points)\n",
    "    \n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log [https://drive.google.com/file/d/1tqMptjHvAisN1CJ35oCEZ9_lb0cEJwV0/view].\n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter 𝑎uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Random Click Model (RCM)\n",
    "Any document can be clicked with the same (constant) probability p\n",
    "p = number of clicks / number of show documents,\n",
    "and will be learned using the Yandex Click Log\n",
    "'''\n",
    "def random_click_model(documents, p):\n",
    "    clicks = []\n",
    "    for i in range(len(documents)):\n",
    "        if random.uniform(0, 1) <= p:\n",
    "            clicks.append(i)\n",
    "    return clicks # return list of click indexes\n",
    "\n",
    "test_documents = ('HR', 'N', 'R', 'N', 'N', 'R', 'N', 'N', 'R', 'R')\n",
    "print(random_click_model(test_documents, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Position-Based Model (PBM)\n",
    "Any document can be clicked with a probability that is calculated\n",
    "by multiplying:\n",
    "- Examination (E_r_u = 1):  the probability of examining a document \n",
    "- Attractiveness (A_u = 1): the probability of an attractive document\n",
    "These parameters will be learned using the Yandex Click Log\n",
    "'''\n",
    "def position_based_model(documents, examinations, attracts):\n",
    "    clicks = []\n",
    "    for i in range(len(documents)):\n",
    "        p = examinations[i] * attracts[i]\n",
    "        if random.uniform(0, 1) <= p:\n",
    "            clicks.append(i)\n",
    "    return clicks # return list of click indexes\n",
    "\n",
    "test_documents = ('HR', 'N', 'R', 'N', 'N', 'R', 'N', 'N', 'R', 'R')\n",
    "test_examinations = [1, 0.8, 0.6, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "test_attracts =     [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "print(position_based_model(test_documents, test_examinations, test_attracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Session:\n",
    "    \n",
    "    def __init__(self, elements=None):\n",
    "        if elements != None:\n",
    "            self.session_id = elements[0]\n",
    "            self.time_passed = elements[1]\n",
    "            self.type_of_action = elements[2]\n",
    "            \n",
    "            if self.type_of_action == 'Q': # Query action\n",
    "                self.query_id = elements[3]\n",
    "                self.region_id = elements[4]\n",
    "                self.list_of_urls = elements[5:]\n",
    "            elif self.type_of_action == 'C': # Click action\n",
    "                self.url_id = elements[3]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'action: ' + self.type_of_action + ', id: ' + self.session_id                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset:\n",
    "# QUERY: SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "# CLICK: SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "# load dataset into list\n",
    "\n",
    "all_sessions = []\n",
    "\n",
    "with open('YandexRelPredChallenge.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        elements = line.split(\"\\n\")[0].split('\\t')\n",
    "        all_sessions += [Session(elements)]\n",
    "        '''\n",
    "        sessionID = elements[0]\n",
    "        timePassed = elements[1]\n",
    "        typeOfAction = elements[2]\n",
    "        \n",
    "        if typeOfAction == 'Q': # query\n",
    "            queryID = elements[3]\n",
    "            regionID = elements[4]\n",
    "            listOfURLs = elements[5:]\n",
    "        elif typeOfAction == 'C': # click\n",
    "            URLID = elements[3]\n",
    "        else:\n",
    "            print(\"ACTION ERROR\")\n",
    "        '''\n",
    "            \n",
    "print(all_sessions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['0',\n",
       "  '0',\n",
       "  'Q',\n",
       "  '8',\n",
       "  '0',\n",
       "  '7',\n",
       "  '103',\n",
       "  '51',\n",
       "  '92',\n",
       "  '43',\n",
       "  '12',\n",
       "  '73',\n",
       "  '69',\n",
       "  '27',\n",
       "  '105'],\n",
       " ['0',\n",
       "  '36',\n",
       "  'Q',\n",
       "  '174',\n",
       "  '0',\n",
       "  '1625',\n",
       "  '1627',\n",
       "  '1623',\n",
       "  '1626',\n",
       "  '1624',\n",
       "  '1622',\n",
       "  '1619',\n",
       "  '1621',\n",
       "  '1620',\n",
       "  '1618'],\n",
       " ['0',\n",
       "  '50',\n",
       "  'Q',\n",
       "  '227',\n",
       "  '0',\n",
       "  '2094',\n",
       "  '2091',\n",
       "  '2087',\n",
       "  '2089',\n",
       "  '2093',\n",
       "  '2088',\n",
       "  '2090',\n",
       "  '2092',\n",
       "  '2095',\n",
       "  '2086'],\n",
       " ['0',\n",
       "  '515',\n",
       "  'Q',\n",
       "  '174',\n",
       "  '0',\n",
       "  '1625',\n",
       "  '1627',\n",
       "  '1623',\n",
       "  '1626',\n",
       "  '1624',\n",
       "  '1622',\n",
       "  '1619',\n",
       "  '1621',\n",
       "  '1620',\n",
       "  '1618'],\n",
       " ['0',\n",
       "  '524',\n",
       "  'Q',\n",
       "  '1974',\n",
       "  '0',\n",
       "  '17562',\n",
       "  '1627',\n",
       "  '1626',\n",
       "  '1623',\n",
       "  '2091',\n",
       "  '17559',\n",
       "  '17563',\n",
       "  '17558',\n",
       "  '17561',\n",
       "  '17560'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of documents shown: 426520\n",
      "Total amount of clicks:          57348\n",
      "\n",
      "Click probability                0.134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13445559411047547"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate click probability for Random Click Model\n",
    "\n",
    "# (constant) click probability = \n",
    "# total amount of documents shown in all sessions /\n",
    "# total amount of clicks in all sessions\n",
    "\n",
    "def calculate_click_prob(all_sessions, print_trace = False):\n",
    "    documents_shown = 0\n",
    "    clicks_amount = 0\n",
    "    for action in all_sessions:\n",
    "        if action[2] == 'Q':\n",
    "            documents_shown += len(action[5:])\n",
    "        else:\n",
    "            clicks_amount += 1\n",
    "    \n",
    "    click_probability = float(clicks_amount) / documents_shown\n",
    "    if print_trace:\n",
    "        print(\"Total amount of documents shown:\", documents_shown)\n",
    "        print(\"Total amount of clicks:         \", clicks_amount)\n",
    "        print()\n",
    "        print(\"Click probability               \", round(click_probability, 3))\n",
    "    return click_probability\n",
    "          \n",
    "calculate_click_prob(all_sessions, print_trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '0', 'Q', '8', '0', '7', '103', '51', '92', '43', '12', '73', '69', '27', '105'], ['0', '36', 'Q', '174', '0', '1625', '1627', '1623', '1626', '1624', '1622', '1619', '1621', '1620', '1618'], ['0', '50', 'Q', '227', '0', '2094', '2091', '2087', '2089', '2093', '2088', '2090', '2092', '2095', '2086'], ['0', '515', 'Q', '174', '0', '1625', '1627', '1623', '1626', '1624', '1622', '1619', '1621', '1620', '1618'], ['0', '524', 'Q', '1974', '0', '17562', '1627', '1626', '1623', '2091', '17559', '17563', '17558', '17561', '17560']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "TODO (for the Position-Based Model):\n",
    "\n",
    "Use EM to learn both alphas (acttractiveness) and gammas (examination)\n",
    "to be able to learn the probability of clicking.\n",
    "\n",
    "Each has 10 parameters (since we show 10 documents), so we get\n",
    "- 10 parameters for acttractiveness,\n",
    "- 10 parameters for examination, and\n",
    "- 10 probabilities (calculated by multiplying these parameters)\n",
    "*** For example, the probability of clicking document 1 is:\n",
    "p1 = alpha1 (attractiveness of d1) * gamma1 (examination of d1)\n",
    "\n",
    "The alphas and gammas are learned together in EM and\n",
    "then the alphas are discarded in the simulation.\n",
    "\n",
    "\n",
    "Notes from Piazza:\n",
    "\n",
    "The algorithm differs if you consider multi-query sessions.\n",
    "You may assume that each query is a different session.\n",
    "You may ignore pagination or concat the results below the first page.\n",
    "'''\n",
    "\n",
    "#TODO\n",
    "def em_algorithm(all_sessions):\n",
    "    print(all_sessions[:5])\n",
    "    \n",
    "em_algorithm(all_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Simulate Interleaving Experiment (10 points)\n",
    "\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter 𝑎uq. Use the relevance label to assign this parameter by setting 𝑎uq for a document u in the ranked list accordingly. (See Click Models for Web Search, http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Results and Analysis (30 points)\n",
    "\n",
    "Compare the results of the offline experiments (i.e. the values of the 𝛥measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "Use easy to read and comprehend visuals to demonstrate the results;\n",
    "Analyze the results on the basis of\n",
    "the evaluation measure used,\n",
    "the interleaving method used,\n",
    "the click model used.\n",
    "Report and ground your conclusions.\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n",
    "\n",
    "Yandex Click Log File:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "In the case of a Query:\n",
    "\n",
    "SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "In the case of a Click:\n",
    "SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "SessionID - the unique identifier of the user session.\n",
    "TimePassed - the time elapsed since the beginning of the current session in standard time units.\n",
    "TypeOfAction - type of user action. This may be either a query (Q), or a click (C).\n",
    "QueryID - the unique identifier of the request.\n",
    "RegionID - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "URLID - the unique identifier of the document.\n",
    "ListOfURLs - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFrom Piazza:\\n\\nWe want wins per metric and then for the online part\\nwe want proportion of wins per click model and then do a 3 x 2\\n(3 offline deltas per metric, 2 online models) comparisons\\nfor the analysis part.\\n\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "From Piazza:\n",
    "\n",
    "We want wins per metric and then for the online part\n",
    "we want proportion of wins per click model and then do a 3 x 2\n",
    "(3 offline deltas per metric, 2 online models) comparisons\n",
    "for the analysis part.\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
