{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "\n",
    "#### Michelle Appel (10170359)\n",
    "#### Nils Hulzeboch (10749411)\n",
    "#### Yves van Montfort (XXX)\n",
    "\n",
    "#### 11-01-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Part [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hypothesis Testing â€“ The problem of multiple comparisons [ 5 points ]\n",
    "Experimentation in AI often happens like this:\n",
    "A. Modify/Build an algorithm\n",
    "B. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "C. If not significant, go back to step A\n",
    "D. If significant, start writing a paper.\n",
    "Compute the probabilities below  How many hypothesis tests, m, does it take to get to  (with Type I error for each test = Î±):\n",
    "(a) P(mt  h  experiment gives significant result | m experiments lacking power to reject H 0)  ? (b) P(at least one significant result | m experiments lacking power to reject H 0)  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$) = $1 - (1 - \\alpha)^m$ (= $m \\alpha$ when $\\alpha$ is small)\n",
    "\n",
    "Where m is the amount of experiments and $\\alpha$ is the Type I error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) P(at least one significant result | m experiments lacking power to reject $H_0$) = $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Bias and unfairness in Interleaving experiments [ 10 points ]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning 2â„3 of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a\n",
    "situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Part [85 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevances = ('N', 'R', 'HR') # The three relevance classes\n",
    "\n",
    "# All combinations with length 5 of the relevance classes\n",
    "combinations = list(itertools.combinations_with_replacement(relevances, 5))\n",
    "\n",
    "# All permutations per combination\n",
    "permutations = ()\n",
    "for combination in combinations:\n",
    "    permutations += tuple(set(itertools.permutations(combination)))\n",
    "    \n",
    "# Ranking pairs of production P and experimental E\n",
    "ranking_pairs = ()\n",
    "for ranking_p in permutations:\n",
    "    for ranking_e in permutations:\n",
    "        if ranking_p != ranking_e: # If pairs are not the same\n",
    "            ranking_pairs += ((ranking_p, ranking_e),) # Extend list with ranking pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Implement Evaluation Measures (10 points)\n",
    "    \n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection â€“ pay extra attention on how to find this)\n",
    "\n",
    "##### Binary evaluation measures:\n",
    "1. Precision at rank k,\n",
    "2. Recall at rank k,\n",
    "3. Average Precision,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "def precision(ranking, rank=None):\n",
    "    tp = ranking[:rank].count('R') + ranking[:rank].count('HR')\n",
    "    fp = ranking[:rank].count('N')\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "# Recall = TP / (TP + FN)\n",
    "def recall(ranking, no_relevant_documents, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    tp = ranking[:rank].count('R') + ranking[:rank].count('HR')\n",
    "    fn = no_relevant_documents - tp\n",
    "    return tp / (tp + fn)\n",
    "    \n",
    "def recalls(ranking_pair, rank=None):\n",
    "    no_relevant_documents = 0\n",
    "    for ranking in ranking_pair:\n",
    "        no_relevant_documents += ranking.count('R') + ranking.count('HR')\n",
    "    \n",
    "    recalls = ()    \n",
    "    for ranking in ranking_pair:\n",
    "        recalls += (recall(ranking=ranking, no_relevant_documents=no_relevant_documents, rank=rank),)\n",
    "    return recalls\n",
    "\n",
    "\n",
    "# Average precision\n",
    "def average_precision(ranking):\n",
    "    precisions = ()\n",
    "    for rank in range(1, len(ranking)+1):\n",
    "        if ranking[rank-1] == 'R' or ranking[rank-1] == 'HR':\n",
    "            precisions += (precision(ranking, rank=rank),)\n",
    "\n",
    "    if len(precisions) > 0:\n",
    "        return np.mean(precisions)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-graded evaluation measures:\n",
    "\n",
    "1. Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "2. Expected Reciprocal Rank (ERR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "def nDCGk(ranking, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    gains = ()\n",
    "    for r, rel_grade in enumerate(ranking[:rank]):\n",
    "        if rel_grade == 'N':\n",
    "            rel_r = 0\n",
    "        elif rel_grade == 'R':\n",
    "            rel_r = 0.5\n",
    "        elif rel_grade == 'HR':\n",
    "            rel_r = 1\n",
    "        \n",
    "        gains += ((2**rel_r - 1)/(np.log2(2+r)),)\n",
    "        \n",
    "    return np.sum(gains)\n",
    "\n",
    "\n",
    "# Mapping from relevance grades to probability of relevance\n",
    "def R(rel_grade, g_max):\n",
    "    if rel_grade == 'N':\n",
    "        g = 0\n",
    "    elif rel_grade == 'R':\n",
    "        g = 0.5\n",
    "    elif rel_grade == 'HR':\n",
    "        g = 1\n",
    "\n",
    "    return (2**g - 1)/(2**g_max)\n",
    "\n",
    "# Expected Reciprocal Rank (ERR)\n",
    "def ERR(ranking, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking)\n",
    "    \n",
    "    g_max = 1\n",
    "    \n",
    "    likelihood_sum_elem = ()\n",
    "    for r, r_rel_grad in enumerate(ranking[:rank]):\n",
    "        P_prod_elem = ()\n",
    "        for i_rel_grad in ranking[:rank]:\n",
    "            P_prod_elem += ((1/(r+1))*(1 - R(i_rel_grad, g_max))*R(r_rel_grad, g_max),)\n",
    "        likelihood_sum_elem += (np.prod(P_prod_elem),)\n",
    "    \n",
    "    return np.sum(likelihood_sum_elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the ð›¥measure (0 points)\n",
    "    \n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_measure(ranking_pairs, evaluation_measure):\n",
    "    delta_measures = ()\n",
    "    \n",
    "    for ranking_pair in ranking_pairs:\n",
    "        delta_measure = evaluation_measure(ranking_pair[1]) - evaluation_measure(ranking_pair[0])\n",
    "        if delta_measure > 0:\n",
    "            delta_measures += (delta_measure,)\n",
    "            \n",
    "    return np.mean(delta_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Based on Lecture 2]\n",
    "#### Step 4: Implement Interleaving (15 points)\n",
    "\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Interleaving\n",
    "def balanced_interleaving(ranking_pair, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking_pair[0]) + len(ranking_pair[1])\n",
    "    \n",
    "    p_first = random.randint(0,1)\n",
    "    \n",
    "    interleaved_ranking = ()    \n",
    "    for p, e in zip(*ranking_pair):\n",
    "        if p_first:\n",
    "            interleaved_ranking += (p, e)\n",
    "        else:\n",
    "            interleaved_ranking += (e, p)            \n",
    "    \n",
    "    return interleaved_ranking[:rank]\n",
    "       \n",
    "# Team-Draft Interleaving\n",
    "def team_draft_interleaving(ranking_pair, rank=None):\n",
    "    ranking_p = list(ranking_pair[0])\n",
    "    ranking_e = list(ranking_pair[1])\n",
    "                             \n",
    "    if rank is None:\n",
    "        rank = len(ranking_p) + len(ranking_e)\n",
    "        \n",
    "    team_p = ()\n",
    "    team_e = ()\n",
    "        \n",
    "    interleaved_ranking = ()\n",
    "    for i in range(rank):\n",
    "        if (len(team_p) < len(team_e)) or (len(team_p) == len(team_e) and random.randint(0,1)):\n",
    "            rel_grade = ranking_p.pop(0)\n",
    "            interleaved_ranking += (rel_grade,)\n",
    "            team_p += (rel_grade,)\n",
    "        else:\n",
    "            rel_grade = ranking_e.pop(0)\n",
    "            interleaved_ranking += (rel_grade,)\n",
    "            team_e += (rel_grade,)\n",
    "    \n",
    "    return interleaved_ranking[:rank]\n",
    "\n",
    "def probabilistic_interleaving(ranking_pair, rank=None):\n",
    "    if rank is None:\n",
    "        rank = len(ranking_pair[0]) + len(ranking_pair[1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Implement User Clicks Simulation (15 points)\n",
    "    \n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log [https://drive.google.com/file/d/1tqMptjHvAisN1CJ35oCEZ9_lb0cEJwV0/view].\n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter ð‘Žuq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Simulate Interleaving Experiment (10 points)\n",
    "\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter ð‘Žuq. Use the relevance label to assign this parameter by setting ð‘Žuq for a document u in the ranked list accordingly. (See Click Models for Web Search, http://clickmodels.weebly.com/uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Results and Analysis (30 points)\n",
    "\n",
    "Compare the results of the offline experiments (i.e. the values of the ð›¥measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "Use easy to read and comprehend visuals to demonstrate the results;\n",
    "Analyze the results on the basis of\n",
    "the evaluation measure used,\n",
    "the interleaving method used,\n",
    "the click model used.\n",
    "Report and ground your conclusions.\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n",
    "\n",
    "Yandex Click Log File:\n",
    "\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "In the case of a Query:\n",
    "\n",
    "SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "In the case of a Click:\n",
    "SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "SessionID - the unique identifier of the user session.\n",
    "TimePassed - the time elapsed since the beginning of the current session in standard time units.\n",
    "TypeOfAction - type of user action. This may be either a query (Q), or a click (C).\n",
    "QueryID - the unique identifier of the request.\n",
    "RegionID - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "URLID - the unique identifier of the document.\n",
    "ListOfURLs - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
